{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e98463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "/home/friday_code/miniconda3/envs/champs-infer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-19 02:58:32.948985: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-19 02:58:33.017378: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-19 02:58:33.517487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-19 02:58:35.042371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/friday_code/miniconda3/envs/champs-infer/lib/python3.8/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import glob,os\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw, PyMol, rdFMCS\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit import rdBase\n",
    "from deepchem import metrics\n",
    "from IPython.display import Image, display\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dd2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepChemTqdmCallback:\n",
    "    \"\"\"\n",
    "    DeepChem-style callback: called as callback(model, current_step).\n",
    "    Shows a per-epoch tqdm bar (updates once per batch).\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, leave=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.leave = leave\n",
    "        # Try to infer dataset length\n",
    "        try:\n",
    "            self.n = len(dataset)\n",
    "        except Exception:\n",
    "            y = getattr(dataset, \"y\", None)\n",
    "            if hasattr(y, \"shape\"):\n",
    "                self.n = int(y.shape[0])\n",
    "            else:\n",
    "                self.n = None\n",
    "        self.steps = None if self.n is None else math.ceil(self.n / self.batch_size)\n",
    "        self.pbar = None\n",
    "        self.last_epoch = -1\n",
    "\n",
    "    def __call__(self, model, current_step):\n",
    "        \"\"\"\n",
    "        Called by DeepChem as callback(model, current_step) after each batch.\n",
    "        current_step is an integer (global batch count).\n",
    "        \"\"\"\n",
    "        # ensure int\n",
    "        step = int(current_step)\n",
    "\n",
    "        # If we can't infer steps_per_epoch, show an indeterminate progress spinner\n",
    "        if self.steps is None:\n",
    "            if self.pbar is None:\n",
    "                self.pbar = tqdm(total=None, desc=f\"Step {step}\", leave=self.leave)\n",
    "            else:\n",
    "                self.pbar.update(1)\n",
    "            return\n",
    "\n",
    "        # Determine epoch and batch-within-epoch\n",
    "        epoch = step // self.steps\n",
    "        batch_in_epoch = step % self.steps\n",
    "\n",
    "        # If new epoch, close previous bar and open a new one\n",
    "        if epoch != self.last_epoch:\n",
    "            if self.pbar is not None:\n",
    "                try:\n",
    "                    self.pbar.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self.pbar = tqdm(total=self.steps, desc=f\"Epoch {epoch+1}\", leave=self.leave)\n",
    "            self.last_epoch = epoch\n",
    "            # Update bar to current batch (handles possible non-1 step jumps)\n",
    "            # Usually first call in epoch will have batch_in_epoch == 0 -> update by 1\n",
    "            self.pbar.update(batch_in_epoch + 1)\n",
    "            return\n",
    "\n",
    "        # Same epoch: advance by 1 (typical case)\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Call after training to ensure bar closed.\"\"\"\n",
    "        if self.pbar is not None:\n",
    "            try:\n",
    "                self.pbar.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.pbar = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213b29f",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdaef81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffv = pd.read_csv('../Datasets/FFV/FFV.csv')\n",
    "tc = pd.read_csv('../Datasets/Tc/Tc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabf5d3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['FFV', 'Tc']\n",
    "datasets = [ffv, tc]\n",
    "\n",
    "batch_normalize = {\n",
    "    'FFV': False,\n",
    "    'Tc': True\n",
    "}\n",
    "\n",
    "dropouts = {\n",
    "    'FFV': 0.1,\n",
    "    'Tc': 0.2\n",
    "}\n",
    "\n",
    "for j in range(len(datasets)):\n",
    "    datasets[j].to_csv(f\"{targets[j]}.csv\", index=False)\n",
    "    DATASET_FILE = f\"./Datasets/FFV/{targets[j]}.csv\"\n",
    "    MODEL_DIR = 'GNN_Model'\n",
    "\n",
    "    # Featurizerization\n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    loader = dc.data.CSVLoader(tasks=[f\"{targets[j]}\"], feature_field=\"SMILES\", featurizer=featurizer)\n",
    "    dataset = loader.create_dataset(DATASET_FILE, shard_size=10000)\n",
    "    \n",
    "    print(f\"{targets[j]} data loaded successfullyÔºÅ\\n\")\n",
    "    splitter = dc.splits.splitters.RandomSplitter()\n",
    "    trainset, testset = splitter.train_test_split(dataset, frac_train=0.8, seed=1)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    metrics_rmse_train = []\n",
    "    metrics_mae_train = []\n",
    "    metrics_r2_train = []\n",
    "    metrics_rmse_test = []\n",
    "    metrics_mae_test = []\n",
    "    metrics_r2_test = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"Executing: %d/5\" %(i+1))\n",
    "        print(\"#\"*60)\n",
    "        MODEL_DIR = 'GNN_Model'\n",
    "        \n",
    "        if not os.path.exists(MODEL_DIR + '/2 layers/' + 'loop' + str(i+1)):\n",
    "            os.makedirs(MODEL_DIR + '/2 layers/' + 'loop' + str(i+1))\n",
    "        MODEL_DIR = MODEL_DIR + '/2 layers/' + 'loop' + str(i+1)\n",
    "    \n",
    "        model = dc.models.GraphConvModel(1, \n",
    "                  graph_conv_layers=[64,64],\n",
    "                  dense_layer_size = 128,\n",
    "                  mode=\"regression\",\n",
    "                  batch_normalize = batch_normalize[targets[j]],\n",
    "                  batch_size=20,\n",
    "                  model_dir=MODEL_DIR,\n",
    "                  dropout=dropouts[targets[j]])\n",
    "        \n",
    "        batch_size = 20\n",
    "        tqcb = DeepChemTqdmCallback(trainset, batch_size=batch_size, leave=False)\n",
    "\n",
    "        # Fit\n",
    "        model.fit(trainset, nb_epoch=1000, callbacks=[tqcb])\n",
    "        tqcb.close()\n",
    "        \n",
    "        # Predict\n",
    "        test_pred = model.predict(testset)\n",
    "        train_pred = model.predict(trainset)\n",
    "    \n",
    "        # Metrics\n",
    "        rmse = metrics.mean_squared_error(y_true=trainset.y, y_pred=train_pred, squared=False)   # RMSE\n",
    "        r2 = metrics.r2_score(y_true=trainset.y, y_pred=train_pred)\n",
    "        mae = metrics.mean_absolute_error(y_true=trainset.y, y_pred=train_pred)\n",
    "    \n",
    "        rmse_test = metrics.mean_squared_error(y_true=testset.y, y_pred=test_pred, squared=False)   # RMSE\n",
    "        r2_test = metrics.r2_score(y_true=testset.y, y_pred=test_pred)\n",
    "        mae_test = metrics.mean_absolute_error(y_true=testset.y, y_pred=test_pred)\n",
    "    \n",
    "        metrics_r2_train.append(r2)\n",
    "        metrics_rmse_train.append(rmse)\n",
    "        metrics_mae_train.append(mae)\n",
    "        metrics_r2_test.append(r2_test)\n",
    "        metrics_rmse_test.append(rmse_test)\n",
    "        metrics_mae_test.append(mae_test)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Time cost for GNN on polymer dataset: %.3f min\" % ((end-start)/60))\n",
    "    \n",
    "    print(\"Train_R2: %.2f (+/- %.2f)\" % (np.mean(metrics_r2_train), np.std(metrics_r2_train)))\n",
    "    print(\"Train_RMSE: %.2f (+/- %.2f)\" % (np.mean(metrics_rmse_train), np.std(metrics_rmse_train)))\n",
    "    print(\"Train_MAE: %.2f (+/- %.2f)\" % (np.mean(metrics_mae_train), np.std(metrics_mae_train)))\n",
    "    \n",
    "    print(\"Test_R2: %.2f (+/- %.2f)\" % (np.mean(metrics_r2_test), np.std(metrics_r2_test)))\n",
    "    print(\"Test_RMSE: %.2f (+/- %.2f)\" % (np.mean(metrics_rmse_test), np.std(metrics_rmse_test)))\n",
    "    print(\"Test_MAE: %.2f (+/- %.2f)\" % (np.mean(metrics_mae_test), np.std(metrics_mae_test)))\n",
    "\n",
    "    shutil.make_archive(f'{targets[j]}_GNN_Model', 'zip', f'/kaggle/working/GNN_Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "champs-infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
