{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de794e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from rdkit import Chem\n",
    "import random\n",
    "import math\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "\n",
    "def compute_weighted_score(per_target_maes: dict, sample_counts: dict):\n",
    "    \"\"\"\n",
    "    per_target_maes: {'Tg': mae_value, 'FFV':..}\n",
    "    sample_counts: {'Tg': N_tg, ...}\n",
    "    weights = (1/sqrt(N)) normalized to sum 1\n",
    "    \"\"\"\n",
    "    keys = list(per_target_maes.keys())\n",
    "    inv_sqrt = np.array([1.0/math.sqrt(sample_counts[k]) for k in keys])\n",
    "    weights = inv_sqrt / inv_sqrt.sum()\n",
    "    score = sum(per_target_maes[k] * w for k,w in zip(keys, weights))\n",
    "    return score, dict(zip(keys, weights))\n",
    "\n",
    "ATOM_LIST = [\"C\",\"H\",\"O\",\"N\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\"]\n",
    "MAX_DEGREE = 5\n",
    "def atom_features(atom):\n",
    "    at = atom.GetSymbol()\n",
    "    one_hot = [1.0 if at == a else 0.0 for a in ATOM_LIST]\n",
    "    if not any(one_hot): one_hot.append(1.0)\n",
    "    else: one_hot.append(0.0)\n",
    "    charge = [atom.GetFormalCharge()]\n",
    "    aromatic = [1.0 if atom.GetIsAromatic() else 0.0]\n",
    "    deg = atom.GetDegree()\n",
    "    deg_oh = [1.0 if deg==d else 0.0 for d in range(MAX_DEGREE+1)]\n",
    "    feats = one_hot + charge + aromatic + deg_oh\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def mol_to_pyg_data(smiles, global_features=None, y=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    node_feats = [atom_features(a) for a in mol.GetAtoms()]\n",
    "    x = torch.tensor(np.vstack(node_feats), dtype=torch.float)\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for bond in mol.GetBonds():\n",
    "        a1 = bond.GetBeginAtomIdx(); a2 = bond.GetEndAtomIdx()\n",
    "        edge_index.append([a1, a2]); edge_index.append([a2, a1])\n",
    "        bt = bond.GetBondType()\n",
    "        bond_type = [0.0,0.0,0.0,0.0]\n",
    "        if bt == Chem.rdchem.BondType.SINGLE: bond_type[0]=1.0\n",
    "        elif bt == Chem.rdchem.BondType.DOUBLE: bond_type[1]=1.0\n",
    "        elif bt == Chem.rdchem.BondType.TRIPLE: bond_type[2]=1.0\n",
    "        elif bt == Chem.rdchem.BondType.AROMATIC: bond_type[3]=1.0\n",
    "        edge_attr.append(bond_type); edge_attr.append(bond_type)\n",
    "\n",
    "    if len(edge_index)==0:\n",
    "        edge_index = [[0,0]]; edge_attr = [[0,0,0,0]]\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(np.vstack(edge_attr), dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    if global_features is not None:\n",
    "        gf = torch.tensor(global_features, dtype=torch.float)\n",
    "        if gf.dim()==1: gf = gf.unsqueeze(0)  # shape (1, D)\n",
    "        data.global_feats = gf\n",
    "    else:\n",
    "        data.global_feats = torch.zeros(1, dtype=torch.float)\n",
    "    if y is not None:\n",
    "        data.y = torch.tensor([y], dtype=torch.float)\n",
    "    return data\n",
    "\n",
    "def mol_to_scaffold(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None: return None\n",
    "        scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "        return Chem.MolToSmiles(scaffold, isomericSmiles=False)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def scaffold_fold_assignments(df, n_folds=5, smiles_col=\"SMILES\", seed=42):\n",
    "    random.seed(seed)\n",
    "    scaffolds = {}\n",
    "    for idx, smi in enumerate(df[smiles_col].values):\n",
    "        scaf = mol_to_scaffold(smi)\n",
    "        if scaf is None: scaf = f\"EMPTY_{idx}\"\n",
    "        scaffolds.setdefault(scaf, []).append(idx)\n",
    "    groups = sorted(scaffolds.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    fold_sizes = [0]*n_folds\n",
    "    fold_assign = np.zeros(len(df), dtype=int)\n",
    "    for scaf, idxs in groups:\n",
    "        f = int(np.argmin(fold_sizes))\n",
    "        for idx in idxs: fold_assign[idx] = f\n",
    "        fold_sizes[f] += len(idxs)\n",
    "    return fold_assign\n",
    "\n",
    "class GNNWithGlobalFeats(nn.Module):\n",
    "    def __init__(self, node_in_dim, edge_in_dim, global_in_dim,\n",
    "                 gnn_hidden=128, n_gnn_layers=3, mlp_hidden=128, dropout=0.2,\n",
    "                 conv_type='gcn'):  # NEW: conv_type\n",
    "        super().__init__()\n",
    "        self.global_in_dim = global_in_dim\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()  # CHANGED: BatchNorms for node features after conv\n",
    "        in_dim = node_in_dim\n",
    "        for _ in range(n_gnn_layers):\n",
    "            if conv_type == 'gcn':\n",
    "                self.convs.append(GCNConv(in_dim, gnn_hidden))\n",
    "            elif conv_type == 'gat':\n",
    "                # CHANGED: GAT with single head for simplicity\n",
    "                self.convs.append(GATConv(in_dim, gnn_hidden // 1, heads=1, concat=False))\n",
    "            else:\n",
    "                raise ValueError(\"conv_type must be 'gcn' or 'gat'\")\n",
    "            self.bns.append(nn.BatchNorm1d(gnn_hidden))\n",
    "            in_dim = gnn_hidden\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "        total_in = gnn_hidden + global_in_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(total_in, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden//2, 1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)        # CHANGED\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        pooled = self.pool(x, batch)\n",
    "        gfeat = data.global_feats.to(pooled.dtype).to(pooled.device)\n",
    "        # same robust handling as before\n",
    "        if gfeat.dim() == 1:\n",
    "            if gfeat.numel() == self.global_in_dim:\n",
    "                gfeat = gfeat.unsqueeze(0).expand(pooled.size(0), -1)\n",
    "            elif gfeat.numel() == pooled.size(0) * self.global_in_dim:\n",
    "                gfeat = gfeat.view(pooled.size(0), self.global_in_dim)\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected global_feats shape\")\n",
    "        elif gfeat.dim() == 2:\n",
    "            if gfeat.size(0) != pooled.size(0) and gfeat.numel() == pooled.size(0) * self.global_in_dim:\n",
    "                gfeat = gfeat.view(pooled.size(0), self.global_in_dim)\n",
    "            elif gfeat.size(0) != pooled.size(0):\n",
    "                gfeat = gfeat.mean(dim=0, keepdim=True).expand(pooled.size(0), -1)\n",
    "\n",
    "        out = self.mlp(torch.cat([pooled, gfeat], dim=1))\n",
    "        return out.view(-1)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, loss_fn, clip_norm=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred, batch.y.view(-1))\n",
    "        loss.backward()\n",
    "        if clip_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)  # CHANGED\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate_mae_inverse_scaled(model, loader, device, y_scaler):\n",
    "    \"\"\"Evaluate MAE but invert target scaling back to original units before MAE.\"\"\"\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch).detach().cpu().numpy().tolist()\n",
    "            y_batch = batch.y.view(-1).detach().cpu().numpy().tolist()\n",
    "            preds.extend(pred)\n",
    "            ys.extend(y_batch)\n",
    "    # inverse transform (y_scaler expects 2D)\n",
    "    preds = np.array(preds).reshape(-1, 1)\n",
    "    ys = np.array(ys).reshape(-1, 1)\n",
    "    preds_orig = y_scaler.inverse_transform(preds).ravel()\n",
    "    ys_orig = y_scaler.inverse_transform(ys).ravel()\n",
    "    return mean_absolute_error(ys_orig, preds_orig), ys_orig, preds_orig\n",
    "\n",
    "def randomized_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    return Chem.MolToSmiles(mol, doRandom=True)\n",
    "\n",
    "def run_single_train_until_target(csv_path,\n",
    "                                  target_col,\n",
    "                                  descriptor_cols,\n",
    "                                  smiles_col=\"SMILES\",\n",
    "                                  device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                                  seed=42,\n",
    "                                  max_epochs=1000,\n",
    "                                  patience=30,\n",
    "                                  batch_size=32,\n",
    "                                  conv_type='gcn',\n",
    "                                  n_augment_small=1,\n",
    "                                  clip_grad_norm=5.0,\n",
    "                                  target_mae=None,\n",
    "                                  tol_rel=0.05,\n",
    "                                  tol_abs=1e-6,\n",
    "                                  verbose=True):\n",
    "    \"\"\"\n",
    "    Train one model on a single train/val split and stop early when validation MAE\n",
    "    is within tolerance of `target_mae` (or when early-stopping triggers).\n",
    "    Returns saved artifact paths and the achieved val MAE.\n",
    "    \"\"\"\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df[target_col].notna()].reset_index(drop=True)\n",
    "\n",
    "    # Create scaffold folds and pick a single validation fold (fold 0)\n",
    "    fold_assign = scaffold_fold_assignments(df, n_folds=5, smiles_col=smiles_col, seed=seed)\n",
    "    train_idx = [i for i,f in enumerate(fold_assign) if f != 0]\n",
    "    val_idx   = [i for i,f in enumerate(fold_assign) if f == 0]\n",
    "\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # Fit scalers on TRAIN only\n",
    "    desc_scaler = StandardScaler().fit(train_df[descriptor_cols].values.astype(float))\n",
    "    y_scaler = StandardScaler().fit(train_df[[target_col]].values.astype(float))\n",
    "\n",
    "    # Transform\n",
    "    X_train = desc_scaler.transform(train_df[descriptor_cols].values.astype(float))\n",
    "    X_val   = desc_scaler.transform(val_df[descriptor_cols].values.astype(float))\n",
    "    y_train = y_scaler.transform(train_df[[target_col]].values.astype(float)).ravel()\n",
    "    y_val   = y_scaler.transform(val_df[[target_col]].values.astype(float)).ravel()\n",
    "\n",
    "    # Build graph objects\n",
    "    train_data = []\n",
    "    for i in range(len(train_df)):\n",
    "        smi = train_df.loc[i, smiles_col]\n",
    "        d = mol_to_pyg_data(smi, global_features=X_train[i], y=float(y_train[i]))\n",
    "        if d is None:\n",
    "            continue\n",
    "        d.idx = train_idx[i]\n",
    "        d.orig_smiles = smi\n",
    "        train_data.append(d)\n",
    "\n",
    "    # optional augmentation for small train sets\n",
    "    if len(train_data) < 2000 and n_augment_small > 0:\n",
    "        aug_list = []\n",
    "        for d in train_data:\n",
    "            for _ in range(n_augment_small):\n",
    "                rs = randomized_smiles(d.orig_smiles)\n",
    "                if rs is None: continue\n",
    "                aug_d = mol_to_pyg_data(rs, global_features=d.global_feats.detach().cpu().numpy().ravel(), y=d.y.item())\n",
    "                if aug_d is None: continue\n",
    "                aug_d.idx = d.idx\n",
    "                aug_d.orig_smiles = rs\n",
    "                aug_list.append(aug_d)\n",
    "        if aug_list:\n",
    "            train_data += aug_list\n",
    "            if verbose: print(f\"Augmented train set with {len(aug_list)} randomized-smiles.\")\n",
    "\n",
    "    val_data = []\n",
    "    for i in range(len(val_df)):\n",
    "        smi = val_df.loc[i, smiles_col]\n",
    "        d = mol_to_pyg_data(smi, global_features=X_val[i], y=float(y_val[i]))\n",
    "        if d is None:\n",
    "            continue\n",
    "        d.idx = val_idx[i]\n",
    "        d.orig_smiles = smi\n",
    "        val_data.append(d)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = PyGDataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = PyGDataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # model\n",
    "    node_dim = train_data[0].x.shape[1]\n",
    "    edge_dim = train_data[0].edge_attr.shape[1] if hasattr(train_data[0], 'edge_attr') else 0\n",
    "    global_dim = train_data[0].global_feats.shape[0] if train_data[0].global_feats.dim()==1 else train_data[0].global_feats.shape[1]\n",
    "\n",
    "    model = GNNWithGlobalFeats(node_in_dim=node_dim, edge_in_dim=edge_dim, global_in_dim=global_dim,\n",
    "                               gnn_hidden=128, n_gnn_layers=3, mlp_hidden=128, dropout=0.2,\n",
    "                               conv_type=conv_type).to(device)\n",
    "\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, verbose=verbose, min_lr=1e-6)\n",
    "\n",
    "    best_val_mae = float('inf')\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # target stopping thresholds\n",
    "    stop_enabled = (target_mae is not None)\n",
    "    if stop_enabled:\n",
    "        tol = max(tol_rel * float(target_mae), tol_abs)\n",
    "        if verbose:\n",
    "            print(f\"Target MAE {target_mae} with tolerance {tol} (relative tol {tol_rel}, abs tol {tol_abs})\")\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device, loss_fn, clip_norm=clip_grad_norm)\n",
    "        val_mae, ys_orig, preds_orig = evaluate_mae_inverse_scaled(model, val_loader, device, y_scaler)\n",
    "        scheduler.step(val_mae)\n",
    "\n",
    "        if val_mae < best_val_mae - 1e-6:\n",
    "            best_val_mae = val_mae\n",
    "            best_epoch = epoch\n",
    "            no_improve = 0\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if verbose and (epoch % 5 == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:04d} | train_loss {train_loss:.6f} | val_mae_orig {val_mae:.6f} | best_val {best_val_mae:.6f}\")\n",
    "\n",
    "        # stop if we reached target MAE within tolerance\n",
    "        if stop_enabled and abs(val_mae - float(target_mae)) <= tol:\n",
    "            if verbose:\n",
    "                print(f\"Stopping at epoch {epoch} because val_mae {val_mae:.6f} is within tolerance of target {target_mae}.\")\n",
    "            break\n",
    "\n",
    "        # normal early stopping\n",
    "        if no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch} (best_epoch {best_epoch} | best_val_mae {best_val_mae:.6f})\")\n",
    "            break\n",
    "\n",
    "    # restore best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: best_state[k].to(device) for k in best_state})\n",
    "\n",
    "    # final metric\n",
    "    val_mae_final, ys_orig, preds_orig = evaluate_mae_inverse_scaled(model, val_loader, device, y_scaler)\n",
    "    if verbose:\n",
    "        print(f\"Final val MAE (orig scale): {val_mae_final:.6f} (best_epoch {best_epoch})\")\n",
    "\n",
    "    # save model + scalers (single-model artifacts)\n",
    "    base = f\"{target_col}_single\"\n",
    "    pkg = {\n",
    "        'state_dict': {k: v.cpu().clone() for k, v in model.state_dict().items()},\n",
    "        'node_dim': node_dim, 'edge_dim': edge_dim, 'global_dim': global_dim,\n",
    "        'gnn_hidden': 128, 'n_gnn_layers': 3, 'mlp_hidden': 128, 'dropout': 0.2,\n",
    "        'conv_type': conv_type, 'val_mae_orig': float(val_mae_final)\n",
    "    }\n",
    "    model_path = f\"model_{base}.pt\"\n",
    "    desc_path = f\"desc_scaler_{base}.pkl\"\n",
    "    y_path = f\"y_scaler_{base}.pkl\"\n",
    "    joblib.dump(desc_scaler, desc_path)\n",
    "    joblib.dump(y_scaler, y_path)\n",
    "    torch.save(pkg, model_path)\n",
    "    if verbose:\n",
    "        print(f\"Saved model -> {model_path}; scalers -> {desc_path}, {y_path}\")\n",
    "\n",
    "    return {\n",
    "        'model_path': model_path,\n",
    "        'desc_scaler_path': desc_path,\n",
    "        'y_scaler_path': y_path,\n",
    "        'val_mae': val_mae_final,\n",
    "        'epoch': best_epoch,\n",
    "        'preds_val': preds_orig,\n",
    "        'y_val': ys_orig\n",
    "    }\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dataset_dir = \"/kaggle/input/augmented-polymer-data/results\"\n",
    "#     targets_to_train = {\n",
    "#         # 'Tg': 39.6794,\n",
    "#         # 'FFV': 0.0042,\n",
    "#         # 'Tc': 0.0211,\n",
    "#         'Density': 0.0182,\n",
    "#         'Rg': 1.1638\n",
    "#     }\n",
    "\n",
    "#     results = {}\n",
    "#     for t, goal in targets_to_train.items():\n",
    "#         csv_path = os.path.join(dataset_dir, f\"{t}_data.csv\")\n",
    "#         print(f\"\\n=== Training single-model for {t} aiming MAE ~ {goal} ===\")\n",
    "#         df_tmp = pd.read_csv(csv_path)\n",
    "#         descriptor_cols = df_tmp.drop(columns=[t, 'SMILES']).columns.tolist()\n",
    "#         joblib.dump(descriptor_cols, f\"desc_cols_{t}_single.pkl\")\n",
    "#         res = run_single_train_until_target(csv_path=csv_path,\n",
    "#                                             target_col=t,\n",
    "#                                             descriptor_cols=descriptor_cols,\n",
    "#                                             smiles_col=\"SMILES\",\n",
    "#                                             device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                                             seed=42,\n",
    "#                                             max_epochs=1000,\n",
    "#                                             patience=40,\n",
    "#                                             batch_size=32,\n",
    "#                                             conv_type='gcn',\n",
    "#                                             n_augment_small=1,\n",
    "#                                             clip_grad_norm=5.0,\n",
    "#                                             target_mae=goal,\n",
    "#                                             tol_rel=0.05,\n",
    "#                                             tol_abs=1e-6,\n",
    "#                                             verbose=True)\n",
    "#         results[t] = res\n",
    "#         print(f\"-> {t} achieved val MAE {res['val_mae']:.6f} (target {goal})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_host_train(\n",
    "    target,\n",
    "    host_train_csv,\n",
    "    model_pattern=\"model_{}_fold*.pt\",\n",
    "    desc_cols_file=None,\n",
    "    aggregate=\"mean\",\n",
    "    evaluate=False,\n",
    "):\n",
    "    # load host train\n",
    "    df = pd.read_csv(host_train_csv).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    desc_cols = (\n",
    "        joblib.load(desc_cols_file)\n",
    "        if desc_cols_file\n",
    "        else [c for c in df.columns if c not in [\"SMILES\", target]]\n",
    "    )\n",
    "    # collect models\n",
    "    model_files = sorted(glob.glob(model_pattern.format(target)))\n",
    "    assert model_files, \"No fold models found\"\n",
    "    all_preds = np.zeros((len(model_files), n), dtype=float)\n",
    "    for i, mp in enumerate(model_files):\n",
    "        pkg = torch.load(mp, map_location=\"cpu\")\n",
    "        # load per-fold scalers (must exist)\n",
    "        desc_scaler = joblib.load(pkg[\"scaler_files\"][\"desc\"])\n",
    "        y_scaler = joblib.load(pkg[\"scaler_files\"][\"y\"])\n",
    "        # scale whole-host descriptors using this fold's scaler\n",
    "        X = df[desc_cols].values.astype(float)\n",
    "        Xs = desc_scaler.transform(X)\n",
    "        # build graphs for host rows (order preserved)\n",
    "        data_list = []\n",
    "        for idx in range(n):\n",
    "            smi = df.loc[idx, \"SMILES\"]\n",
    "            d = mol_to_pyg_data(smi, global_features=Xs[idx], y=None)\n",
    "            if d is None:\n",
    "                # fallback single-node graph\n",
    "                from rdkit import Chem\n",
    "\n",
    "                zero = torch.zeros(\n",
    "                    (1, len(atom_features(Chem.Atom(\"C\")))), dtype=torch.float\n",
    "                )\n",
    "                d = Data(\n",
    "                    x=zero,\n",
    "                    edge_index=torch.tensor([[0], [0]], dtype=torch.long),\n",
    "                    edge_attr=torch.zeros((1, 4)),\n",
    "                    global_feats=torch.tensor(Xs[idx], dtype=torch.float),\n",
    "                )\n",
    "            d.orig_idx = torch.tensor(idx, dtype=torch.long)\n",
    "            data_list.append(d)\n",
    "        loader = PyGDataLoader(\n",
    "            data_list, batch_size=64, shuffle=False, num_workers=0\n",
    "        )\n",
    "        # instantiate model and load weights\n",
    "        model = GNNWithGlobalFeats(\n",
    "            node_in_dim=pkg[\"node_dim\"],\n",
    "            edge_in_dim=pkg[\"edge_dim\"],\n",
    "            global_in_dim=pkg[\"global_dim\"],\n",
    "            gnn_hidden=pkg.get(\"gnn_hidden\", 128),\n",
    "            n_gnn_layers=pkg.get(\"n_gnn_layers\", 3),\n",
    "            mlp_hidden=pkg.get(\"mlp_hidden\", 128),\n",
    "            dropout=pkg.get(\"dropout\", 0.2),\n",
    "            conv_type=pkg.get(\"conv_type\", \"gcn\"),\n",
    "        )\n",
    "        model.load_state_dict(pkg[\"state_dict\"])\n",
    "        model.eval()\n",
    "        preds_fold = np.zeros(n, dtype=float)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(\"cpu\")\n",
    "                out = model(batch).detach().cpu().numpy()\n",
    "                if hasattr(batch, \"orig_idx\"):\n",
    "                    idxs = batch.orig_idx.detach().cpu().numpy().ravel()\n",
    "                    for p, idx in zip(out.tolist(), idxs.tolist()):\n",
    "                        preds_fold[int(idx)] = p\n",
    "                else:\n",
    "                    # fallback sequential\n",
    "                    pass\n",
    "        # inverse-scale fold preds to original units\n",
    "        preds_orig = y_scaler.inverse_transform(\n",
    "            preds_fold.reshape(-1, 1)\n",
    "        ).ravel()\n",
    "        all_preds[i, :] = preds_orig\n",
    "\n",
    "    # aggregate\n",
    "    if aggregate == \"mean\":\n",
    "        final = all_preds.mean(axis=0)\n",
    "    else:\n",
    "        final = all_preds.mean(axis=0)  # extendable to weighted\n",
    "\n",
    "    if evaluate:\n",
    "        # compute host-train MAE\n",
    "        host_mae = mean_absolute_error(df[target].values.astype(float), final)\n",
    "        print(\n",
    "            f\"Host-train MAE for {target}: {host_mae:.6f} (using {len(model_files)} fold models)\"\n",
    "        )\n",
    "        return host_mae, final, all_preds\n",
    "    else:\n",
    "        return final, all_preds\n",
    "\n",
    "\n",
    "submission_df = {}\n",
    "for label in [\"Density\", \"Rg\"]:\n",
    "    host_csv = f\"./Datasets/{label}/{label}.csv\"\n",
    "    mae, preds, allp = eval_on_host_train(\n",
    "        label,\n",
    "        host_csv,\n",
    "        model_pattern=f\"model_{label}_fold*.pt\",\n",
    "        desc_cols_file=f\"desc_cols_{label}.pkl\",\n",
    "        evaluate=True,\n",
    "    )\n",
    "    submission_df[label] = preds\n",
    "\n",
    "print(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
